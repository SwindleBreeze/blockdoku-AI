{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a075caff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from tqdm import tqdm # Import tqdm\n",
    "\n",
    "import settings as s\n",
    "from blockdoku_env import BlockdokuEnv\n",
    "from dqn_agent import DQNAgent # PyTorch version\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7415528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grid_formatted(valid_action_mask):\n",
    "    \"\"\"\n",
    "    Print the valid action mask as a 2D grid for each piece.\n",
    "    Assumes action encoding: piece_idx * (GRID_HEIGHT * GRID_WIDTH) + row * GRID_WIDTH + col\n",
    "    \"\"\"\n",
    "    # Assuming 9x9 grid dimensions\n",
    "    grid_height, grid_width = 9, 9\n",
    "    \n",
    "    # Calculate number of pieces based on mask length\n",
    "    total_actions = len(valid_action_mask)\n",
    "    actions_per_piece = grid_height * grid_width\n",
    "    num_pieces = total_actions // actions_per_piece\n",
    "    \n",
    "    # For each piece\n",
    "    for piece_idx in range(num_pieces):\n",
    "        print(f\"\\nValid positions for piece {piece_idx}:\")\n",
    "        \n",
    "        # Create a 2D grid for this piece\n",
    "        piece_grid = np.zeros((grid_height, grid_width), dtype=int)\n",
    "        \n",
    "        # Fill the grid with valid actions\n",
    "        for row in range(grid_height):\n",
    "            for col in range(grid_width):\n",
    "                action_idx = piece_idx * actions_per_piece + row * grid_width + col\n",
    "                if action_idx < total_actions:\n",
    "                    piece_grid[row, col] = 1 if valid_action_mask[action_idx] else 0\n",
    "        \n",
    "        # Print the grid with nice formatting\n",
    "        for row in range(grid_height):\n",
    "            if row > 0 and row % 3 == 0:\n",
    "                print(\"-\" * 21)  # Horizontal separator every 3 rows\n",
    "            \n",
    "            row_str = \"\"\n",
    "            for col in range(grid_width):\n",
    "                row_str += f\"{piece_grid[row, col]} \"\n",
    "                if (col + 1) % 3 == 0 and col < grid_width - 1:\n",
    "                    row_str += \"| \"  # Vertical separator every 3 columns\n",
    "            \n",
    "            print(row_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ea817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Initializing new model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = BlockdokuEnv(render_mode=None)\n",
    "vis_env = None\n",
    "\n",
    "\n",
    "grid_shape_numpy = (s.GRID_HEIGHT, s.GRID_WIDTH, s.STATE_GRID_CHANNELS)\n",
    "piece_vector_size = s.STATE_PIECE_VECTOR_SIZE\n",
    "# Pass numpy shape (H, W, C) to agent, it handles internal PyTorch shape\n",
    "agent = DQNAgent(grid_shape_numpy, piece_vector_size, env.action_size,\n",
    "                    load_model_path=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ea151f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.]\n",
      "[0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.]\n",
      "[0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.]\n",
      "[0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.]\n",
      "[0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.]\n",
      "[0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.]\n",
      "[0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.]\n",
      "[0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.]\n",
      "[0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.] [0.]\n",
      "Initial info: ['u_shape', 'corner_small', 'big_square']\n",
      "\n",
      "Valid positions for piece 0:\n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "---------------------\n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "---------------------\n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "0 0 0 | 0 0 0 | 0 0 0 \n",
      "\n",
      "Valid positions for piece 1:\n",
      "1 1 1 | 1 1 1 | 1 1 0 \n",
      "1 1 1 | 1 1 1 | 1 1 0 \n",
      "1 1 1 | 1 1 1 | 1 1 0 \n",
      "---------------------\n",
      "1 1 1 | 1 1 1 | 1 1 0 \n",
      "1 1 1 | 1 1 1 | 1 1 0 \n",
      "1 1 1 | 1 1 1 | 1 1 0 \n",
      "---------------------\n",
      "1 1 1 | 1 1 1 | 1 1 0 \n",
      "1 1 1 | 1 1 1 | 1 1 0 \n",
      "0 0 0 | 0 0 0 | 0 0 0 \n",
      "\n",
      "Valid positions for piece 2:\n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "---------------------\n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "---------------------\n",
      "1 1 1 | 1 1 1 | 1 0 0 \n",
      "0 0 0 | 0 0 0 | 0 0 0 \n",
      "0 0 0 | 0 0 0 | 0 0 0 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for episode in range(1): # Iterate through the tqdm progress bar\n",
    "state_np, info = env.reset()\n",
    "\n",
    "grid = state_np[\"grid\"]\n",
    "for row in grid:\n",
    "    # Format each cell with fixed width and join them\n",
    "    print(\" \".join([f\"{cell}\" for cell in row]))\n",
    "print(\"Initial info:\", info[\"available_piece_keys\"])\n",
    "# print(\"Initial info:\", info[\"valid_action_mask\"])\n",
    "# print(info[\"valid_action_mask\"])\n",
    "print_grid_formatted(info[\"valid_action_mask\"])\n",
    "episode_score = 0\n",
    "total_episode_loss = 0\n",
    "learn_steps = 0\n",
    "steps = 0\n",
    "done = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022de521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while not done:\n",
    "    # ... (inner loop logic: act, step, remember, replay) ...\n",
    "    valid_mask = info.get(\"valid_action_mask\", None)\n",
    "    print(f\"Valid action mask: {valid_mask}\")\n",
    "    action = agent.act(state_np, valid_action_mask=valid_mask, use_epsilon=True)\n",
    "    print(f\"Action taken: {action}\")\n",
    "    next_state_np, reward, done, info = env.step(action) #\n",
    "    print(f\"Next state: {next_state_np}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "    agent.remember(state_np, action, reward, next_state_np, done)\n",
    "    loss = agent.replay()\n",
    "\n",
    "    state_np = next_state_np\n",
    "    episode_score += reward\n",
    "    if loss > 0:\n",
    "        total_episode_loss += loss\n",
    "        learn_steps += 1\n",
    "    steps += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for episode in range(1): # Iterate through the tqdm progress bar\n",
    "    state_np, info = env.reset()\n",
    "    \n",
    "    grid = state_np[\"grid\"]\n",
    "    for row in grid:\n",
    "        # Format each cell with fixed width and join them\n",
    "        print(\" \".join([f\"{cell}\" for cell in row]))\n",
    "    print(\"Initial info:\", info[\"available_piece_keys\"])\n",
    "    # print(\"Initial info:\", info[\"valid_action_mask\"])\n",
    "    print(info[\"valid_action_mask\"])\n",
    "    print_grid_formatted(info[\"valid_action_mask\"])\n",
    "    episode_score = 0\n",
    "    total_episode_loss = 0\n",
    "    learn_steps = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "        # ... (inner loop logic: act, step, remember, replay) ...\n",
    "        valid_mask = info.get(\"valid_action_mask\", None)\n",
    "        print(f\"Valid action mask: {valid_mask}\")\n",
    "        action = agent.act(state_np, valid_action_mask=valid_mask, use_epsilon=True)\n",
    "        print(f\"Action taken: {action}\")\n",
    "        next_state_np, reward, done, info = env.step(action) #\n",
    "        print(f\"Next state: {next_state_np}, Reward: {reward}, Done: {done}\")\n",
    "\n",
    "        agent.remember(state_np, action, reward, next_state_np, done)\n",
    "        loss = agent.replay()\n",
    "\n",
    "        state_np = next_state_np\n",
    "        episode_score += reward\n",
    "        if loss > 0:\n",
    "            total_episode_loss += loss\n",
    "            learn_steps += 1\n",
    "        steps += 1\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
